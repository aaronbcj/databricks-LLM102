{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "87482f99-0b05-41f2-ab65-7503e22dc0fc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3b9ba9df-5a9b-4020-a131-5db37c8f9922",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Low-Rank Adaption (LoRA)\n",
    "This lab introduces how to apply low-rank adaptation (LoRA) to your model of choice using [Parameter-Efficient Fine-Tuning (PEFT) library developed by Hugging Face](https://huggingface.co/docs/peft/index). \n",
    "\n",
    "\n",
    "### ![Dolly](https://files.training.databricks.com/images/llm/dolly_small.png) Learning Objectives\n",
    "1. Apply LoRA to a model\n",
    "1. Fine-tune on your provided dataset\n",
    "1. Save your model\n",
    "1. Conduct inference using the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63fc1c67-e848-41c4-bf6d-45edaf54a99a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\nCollecting peft==0.4.0\n  Downloading peft-0.4.0-py3-none-any.whl (72 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 72.9/72.9 kB 2.1 MB/s eta 0:00:00\nRequirement already satisfied: transformers in /databricks/python3/lib/python3.10/site-packages (from peft==0.4.0) (4.30.2)\nRequirement already satisfied: accelerate in /databricks/python3/lib/python3.10/site-packages (from peft==0.4.0) (0.20.3)\nRequirement already satisfied: safetensors in /databricks/python3/lib/python3.10/site-packages (from peft==0.4.0) (0.3.2)\nRequirement already satisfied: packaging>=20.0 in /databricks/python3/lib/python3.10/site-packages (from peft==0.4.0) (21.3)\nRequirement already satisfied: pyyaml in /databricks/python3/lib/python3.10/site-packages (from peft==0.4.0) (6.0)\nRequirement already satisfied: torch>=1.13.0 in /databricks/python3/lib/python3.10/site-packages (from peft==0.4.0) (1.13.1+cpu)\nRequirement already satisfied: numpy>=1.17 in /databricks/python3/lib/python3.10/site-packages (from peft==0.4.0) (1.21.5)\nRequirement already satisfied: psutil in /databricks/python3/lib/python3.10/site-packages (from peft==0.4.0) (5.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /databricks/python3/lib/python3.10/site-packages (from packaging>=20.0->peft==0.4.0) (3.0.9)\nRequirement already satisfied: typing-extensions in /databricks/python3/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.4.0) (4.3.0)\nRequirement already satisfied: filelock in /databricks/python3/lib/python3.10/site-packages (from transformers->peft==0.4.0) (3.6.0)\nRequirement already satisfied: tqdm>=4.27 in /databricks/python3/lib/python3.10/site-packages (from transformers->peft==0.4.0) (4.64.1)\nRequirement already satisfied: regex!=2019.12.17 in /databricks/python3/lib/python3.10/site-packages (from transformers->peft==0.4.0) (2022.7.9)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /databricks/python3/lib/python3.10/site-packages (from transformers->peft==0.4.0) (0.13.3)\nRequirement already satisfied: requests in /databricks/python3/lib/python3.10/site-packages (from transformers->peft==0.4.0) (2.28.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /databricks/python3/lib/python3.10/site-packages (from transformers->peft==0.4.0) (0.16.4)\nRequirement already satisfied: fsspec in /databricks/python3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers->peft==0.4.0) (2022.7.1)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.10/site-packages (from requests->transformers->peft==0.4.0) (3.3)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.10/site-packages (from requests->transformers->peft==0.4.0) (2022.9.14)\nRequirement already satisfied: charset-normalizer<3,>=2 in /databricks/python3/lib/python3.10/site-packages (from requests->transformers->peft==0.4.0) (2.0.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /databricks/python3/lib/python3.10/site-packages (from requests->transformers->peft==0.4.0) (1.26.11)\nInstalling collected packages: peft\nSuccessfully installed peft-0.4.0\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install peft==0.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eda23989-89cc-4bf1-8f2c-08614536e71d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resetting the learning environment:\n| enumerating serving endpoints...found 7...(0 seconds)\n| No action taken\n\nSkipping install of existing datasets to \"dbfs:/mnt/dbacademy-datasets/llm-foundation-models/v01-raw\"\n\nValidating the locally installed datasets:\n| listing local files...(3 seconds)\n| removing extra path: /datasets/Abirate___json/Abirate--english_quotes-6e72855d06356857/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/...(0 seconds)\n| removing extra path: /datasets/downloads/...(1 seconds)\n| removing extra file: /datasets/Abirate___json/Abirate--english_quotes-6e72855d06356857/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96.incomplete_info.lock...(0 seconds)\n| removing extra file: /datasets/Abirate___json/Abirate--english_quotes-6e72855d06356857/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96_builder.lock...(0 seconds)\n| removing extra file: /datasets/_dbfs_mnt_dbacademy-datasets_llm-foundation-models_v01-raw_datasets_Abirate___json_Abirate--english_quotes-6e72855d06356857_0.0.0_8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96.lock...(0 seconds)\n| removing extra file: /datasets/_local_disk0_datasets_Abirate___json_Abirate--english_quotes-6e72855d06356857_0.0.0_8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96.lock...(0 seconds)\n| fixed 6 issues...(4 seconds total)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing lab testing framework.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nUsing the \"default\" schema.\n\nPredefined paths variables:\n| DA.paths.working_dir: /dbfs/mnt/dbacademy-users/labuser5846614@vocareum.com/llm-foundation-models\n| DA.paths.user_db:     dbfs:/mnt/dbacademy-users/labuser5846614@vocareum.com/llm-foundation-models/database.db\n| DA.paths.datasets:    /dbfs/mnt/dbacademy-datasets/llm-foundation-models/v01-raw\n\nSetup completed (18 seconds)\n\nThe models developed or used in this course are for demonstration and learning purposes only.\nModels may occasionally output offensive, inaccurate, biased information, or harmful instructions.\n"
     ]
    }
   ],
   "source": [
    "%run ../Includes/Classroom-Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "37afc2ff-ce1c-424e-a024-70bf364bf3ab",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We will re-use the same dataset and model from the demo notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a5178b5-06f9-4ae4-94b6-689fa00b5681",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "652c1d78e7404a4a8c7edad349a9f6eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/222 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5901ab0bd3234e4aac3cecadb54f1412",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/14.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e68bddfa3a3e45c68e829daa61615387",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/85.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8585575a9af448a8ea79453b3c896ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/715 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f2aa336a82e49f1acc0c137ae77fc76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python_shell/dbruntime/huggingface_patches/datasets.py:13: UserWarning: During large dataset downloads, there could be multiple progress bar widgets that can cause performance issues for your notebook or browser. To avoid these issues, use `datasets.utils.logging.disable_progress_bar()` to turn off the progress bars.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63f110db9630489f92dfdb78a895db64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/5.55k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python_shell/dbruntime/huggingface_patches/datasets.py:109: UserWarning: The dataset would be saved to both local disk and DBFS for better performance.\n  warnings.warn(\"The dataset would be saved to both local disk and DBFS for better performance.\")\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/Abirate--english_quotes to /local_disk0/datasets/Abirate___json/Abirate--english_quotes-6e72855d06356857/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f88ff27a7a134e41993ddb63b22091c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7d34b7f2cba4d31965fbdc3aecadfd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/647k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e38f8508d0cd40fe9a5dd2b5e601a53f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0faa96b8347f415eb809a0476f68dc9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /local_disk0/datasets/Abirate___json/Abirate--english_quotes-6e72855d06356857/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89356846db574544ab8a9ebe623232a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c25685dfdc74e3c9f53bfea3e256b75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2508 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['quote', 'author', 'tags', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 50\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"bigscience/bloomz-560m\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "foundation_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "data = load_dataset(\"Abirate/english_quotes\", cache_dir=DA.paths.datasets+\"/datasets\")\n",
    "data = data.map(lambda samples: tokenizer(samples[\"quote\"]), batched=True)\n",
    "train_sample = data[\"train\"].select(range(50))\n",
    "display(train_sample) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e430e4b8-33a3-4a84-a650-64ec1264825e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Define LoRA configurations\n",
    "\n",
    "By using LoRA, you are unfreezing the attention `Weight_delta` matrix and only updating `W_a` and `W_b`. \n",
    "\n",
    "<img src=\"https://files.training.databricks.com/images/llm/lora.png\" width=300>\n",
    "\n",
    "You can treat `r` (rank) as a hyperparameter. Recall from the lecture that, LoRA can perform well with very small ranks based on [Hu et a 2021's paper](https://arxiv.org/abs/2106.09685). GPT-3's validation accuracies across tasks with ranks from 1 to 64 are quite similar. From [PyTorch Lightning's documentation](https://lightning.ai/pages/community/article/lora-llm/):\n",
    "\n",
    "> A smaller r leads to a simpler low-rank matrix, which results in fewer parameters to learn during adaptation. This can lead to faster training and potentially reduced computational requirements. However, with a smaller r, the capacity of the low-rank matrix to capture task-specific information decreases. This may result in lower adaptation quality, and the model might not perform as well on the new task compared to a higher r.\n",
    "\n",
    "Other arguments:\n",
    "- `lora_dropout`: \n",
    "  - Dropout is a regularization method that reduces overfitting by randomly and temporarily removing nodes during training. \n",
    "  - It works like this: <br>\n",
    "    * Apply to most type of layers (e.g. fully connected, convolutional, recurrent) and larger networks\n",
    "    * Temporarily and randomly remove nodes and their connections during each training cycle\n",
    "    ![](https://files.training.databricks.com/images/nn_dropout.png)\n",
    "    * See the original paper here: <a href=\"http://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf\" target=\"_blank\">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</a>\n",
    "- `target_modules`:\n",
    "  - Specifies the module names to apply to \n",
    "  - This is dependent on how the foundation model names its attention weight matrices. \n",
    "  - Typically, this can be:\n",
    "    - `query`, `q`, `q_proj` \n",
    "    - `key`, `k`, `k_proj` \n",
    "    - `value`, `v` , `v_proj` \n",
    "    - `query_key_value` \n",
    "  - The easiest way to inspect the module/layer names is to print the model, like we are doing below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cacbc725-48f3-4d42-87c4-0f9136419cc5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BloomForCausalLM(\n  (transformer): BloomModel(\n    (word_embeddings): Embedding(250880, 1024)\n    (word_embeddings_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n    (h): ModuleList(\n      (0): BloomBlock(\n        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (self_attention): BloomAttention(\n          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n          (attention_dropout): Dropout(p=0.0, inplace=False)\n        )\n        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (mlp): BloomMLP(\n          (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n          (gelu_impl): BloomGelu()\n          (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n      )\n      (1): BloomBlock(\n        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (self_attention): BloomAttention(\n          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n          (attention_dropout): Dropout(p=0.0, inplace=False)\n        )\n        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (mlp): BloomMLP(\n          (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n          (gelu_impl): BloomGelu()\n          (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n      )\n      (2): BloomBlock(\n        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (self_attention): BloomAttention(\n          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n          (attention_dropout): Dropout(p=0.0, inplace=False)\n        )\n        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (mlp): BloomMLP(\n          (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n          (gelu_impl): BloomGelu()\n          (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n      )\n      (3): BloomBlock(\n        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (self_attention): BloomAttention(\n          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n          (attention_dropout): Dropout(p=0.0, inplace=False)\n        )\n        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (mlp): BloomMLP(\n          (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n          (gelu_impl): BloomGelu()\n          (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n      )\n      (4): BloomBlock(\n        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (self_attention): BloomAttention(\n          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n          (attention_dropout): Dropout(p=0.0, inplace=False)\n        )\n        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (mlp): BloomMLP(\n          (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n          (gelu_impl): BloomGelu()\n          (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n      )\n      (5): BloomBlock(\n        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (self_attention): BloomAttention(\n          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n          (attention_dropout): Dropout(p=0.0, inplace=False)\n        )\n        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (mlp): BloomMLP(\n          (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n          (gelu_impl): BloomGelu()\n          (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n      )\n      (6): BloomBlock(\n        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (self_attention): BloomAttention(\n          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n          (attention_dropout): Dropout(p=0.0, inplace=False)\n        )\n        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (mlp): BloomMLP(\n          (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n          (gelu_impl): BloomGelu()\n          (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n      )\n      (7): BloomBlock(\n        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (self_attention): BloomAttention(\n          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n          (attention_dropout): Dropout(p=0.0, inplace=False)\n        )\n        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (mlp): BloomMLP(\n          (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n          (gelu_impl): BloomGelu()\n          (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n      )\n      (8): BloomBlock(\n        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (self_attention): BloomAttention(\n          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n          (attention_dropout): Dropout(p=0.0, inplace=False)\n        )\n        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (mlp): BloomMLP(\n          (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n          (gelu_impl): BloomGelu()\n          (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n      )\n      (9): BloomBlock(\n        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (self_attention): BloomAttention(\n          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n          (attention_dropout): Dropout(p=0.0, inplace=False)\n        )\n        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (mlp): BloomMLP(\n          (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n          (gelu_impl): BloomGelu()\n          (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n      )\n      (10): BloomBlock(\n        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (self_attention): BloomAttention(\n          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n          (attention_dropout): Dropout(p=0.0, inplace=False)\n        )\n        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (mlp): BloomMLP(\n          (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n          (gelu_impl): BloomGelu()\n          (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n      )\n      (11): BloomBlock(\n        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (self_attention): BloomAttention(\n          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n          (attention_dropout): Dropout(p=0.0, inplace=False)\n        )\n        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (mlp): BloomMLP(\n          (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n          (gelu_impl): BloomGelu()\n          (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n      )\n      (12): BloomBlock(\n        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (self_attention): BloomAttention(\n          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n          (attention_dropout): Dropout(p=0.0, inplace=False)\n        )\n        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (mlp): BloomMLP(\n          (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n          (gelu_impl): BloomGelu()\n          (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n      )\n      (13): BloomBlock(\n        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (self_attention): BloomAttention(\n          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n          (attention_dropout): Dropout(p=0.0, inplace=False)\n        )\n        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (mlp): BloomMLP(\n          (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n          (gelu_impl): BloomGelu()\n          (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n      )\n      (14): BloomBlock(\n        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (self_attention): BloomAttention(\n          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n          (attention_dropout): Dropout(p=0.0, inplace=False)\n        )\n        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (mlp): BloomMLP(\n          (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n          (gelu_impl): BloomGelu()\n          (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n      )\n      (15): BloomBlock(\n        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (self_attention): BloomAttention(\n          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n          (attention_dropout): Dropout(p=0.0, inplace=False)\n        )\n        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (mlp): BloomMLP(\n          (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n          (gelu_impl): BloomGelu()\n          (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n      )\n      (16): BloomBlock(\n        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (self_attention): BloomAttention(\n          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n          (attention_dropout): Dropout(p=0.0, inplace=False)\n        )\n        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (mlp): BloomMLP(\n          (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n          (gelu_impl): BloomGelu()\n          (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n      )\n      (17): BloomBlock(\n        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (self_attention): BloomAttention(\n          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n          (attention_dropout): Dropout(p=0.0, inplace=False)\n        )\n        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (mlp): BloomMLP(\n          (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n          (gelu_impl): BloomGelu()\n          (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n      )\n      (18): BloomBlock(\n        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (self_attention): BloomAttention(\n          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n          (attention_dropout): Dropout(p=0.0, inplace=False)\n        )\n        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (mlp): BloomMLP(\n          (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n          (gelu_impl): BloomGelu()\n          (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n      )\n      (19): BloomBlock(\n        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (self_attention): BloomAttention(\n          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n          (attention_dropout): Dropout(p=0.0, inplace=False)\n        )\n        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (mlp): BloomMLP(\n          (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n          (gelu_impl): BloomGelu()\n          (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n      )\n      (20): BloomBlock(\n        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (self_attention): BloomAttention(\n          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n          (attention_dropout): Dropout(p=0.0, inplace=False)\n        )\n        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (mlp): BloomMLP(\n          (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n          (gelu_impl): BloomGelu()\n          (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n      )\n      (21): BloomBlock(\n        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (self_attention): BloomAttention(\n          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n          (attention_dropout): Dropout(p=0.0, inplace=False)\n        )\n        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (mlp): BloomMLP(\n          (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n          (gelu_impl): BloomGelu()\n          (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n      )\n      (22): BloomBlock(\n        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (self_attention): BloomAttention(\n          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n          (attention_dropout): Dropout(p=0.0, inplace=False)\n        )\n        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (mlp): BloomMLP(\n          (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n          (gelu_impl): BloomGelu()\n          (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n      )\n      (23): BloomBlock(\n        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (self_attention): BloomAttention(\n          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n          (attention_dropout): Dropout(p=0.0, inplace=False)\n        )\n        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (mlp): BloomMLP(\n          (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n          (gelu_impl): BloomGelu()\n          (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n      )\n    )\n    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=1024, out_features=250880, bias=False)\n)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Print the model architecture to inspect module names\n",
    "print(foundation_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e406a354-4671-4ad0-bf7b-aa1899171665",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Question 1\n",
    "\n",
    "Fill in `r=1` and `target_modules`. \n",
    "\n",
    "Note:\n",
    "- For `r`, any number is valid. The smaller the r is, the fewer parameters there are to update during the fine-tuning process.\n",
    "\n",
    "Hint: \n",
    "- For `target_modules`, what's the name of the **first** module within each `BloomBlock`'s `self_attention`? \n",
    "\n",
    "Read more about [`LoraConfig` here](https://huggingface.co/docs/peft/conceptual_guides/lora#common-lora-parameters-in-peft)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0984c8a5-4c16-4ac3-a596-24ff056f3865",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "import peft\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=1,\n",
    "    lora_alpha=1, # a scaling factor that adjusts the magnitude of the weight matrix. Usually set to 1\n",
    "    target_modules=[\"query_key_value\"],\n",
    "    lora_dropout=0.05, \n",
    "    bias=\"none\", # this specifies if the bias parameter should be trained. \n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5aa3d83e-5f38-4375-8235-a4eae322aea5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32mPASSED\u001B[0m: All tests passed for lesson2, question1\n\u001B[32mRESULTS RECORDED\u001B[0m: Click `Submit` when all questions are completed to log the results.\n"
     ]
    }
   ],
   "source": [
    "# Test your answer. DO NOT MODIFY THIS CELL.\n",
    "\n",
    "dbTestQuestion2_1(lora_config.r, lora_config.target_modules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "852b603f-8022-47cf-a245-71a3238945c8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###  Question 2\n",
    "\n",
    "Add the adapter layers to the foundation model to be trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "391dc53f-56ee-44be-ba9a-aef8f87977ec",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 98,304 || all params: 559,312,896 || trainable%: 0.01757585078102687\nNone\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "peft_model = get_peft_model(foundation_model, lora_config)\n",
    "print(peft_model.print_trainable_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e4eb585-ebc2-4c01-8b30-0cfcadd64e13",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32mPASSED\u001B[0m: All tests passed for lesson2, question2\n\u001B[32mRESULTS RECORDED\u001B[0m: Click `Submit` when all questions are completed to log the results.\n"
     ]
    }
   ],
   "source": [
    "# Test your answer. DO NOT MODIFY THIS CELL.\n",
    "\n",
    "dbTestQuestion2_2(peft_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6212cab1-763f-4ad9-abfe-5cee34a99ed1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Define `Trainer` class for fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6e4083a0-8c0d-4652-9946-a40ea817ed16",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Question 3 \n",
    "\n",
    "Fill out the `Trainer` class. Feel free to tweak the `training_args` we provided, but remember that lowering the learning rate and increasing the number of epochs will increase training time significantly. If you change none of the defaults we set below, it could take ~15 mins to fine-tune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b25fabc1-21d6-44c9-97fb-50879cdd21cf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/05/19 02:17:20 INFO mlflow.tracking.fluent: Experiment with name '/Users/labuser5846614@vocareum.com/LLM 02L - LoRA with PEFT' does not exist. Creating a new experiment.\n/databricks/python/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\nYou're using a BloomTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='35' max='35' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [35/35 12:53, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TrainOutput(global_step=35, training_loss=6.088029261997768, metrics={'train_runtime': 779.9728, 'train_samples_per_second': 0.321, 'train_steps_per_second': 0.045, 'total_flos': 58346118414336.0, 'train_loss': 6.088029261997768, 'epoch': 5.0})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO\n",
    "import transformers\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import os\n",
    "import mlflow\n",
    "\n",
    "# Tell MLflow Tracking to use this explicit experiment path,\n",
    "# which is located on the left hand sidebar under Machine Learning -> Experiments \n",
    "mlflow.set_experiment(f\"/Users/{DA.username}/LLM 02L - LoRA with PEFT\")\n",
    "\n",
    "output_directory = os.path.join(DA.paths.working_dir, \"peft_lab_outputs\")\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_directory,\n",
    "    auto_find_batch_size=True,\n",
    "    learning_rate= 3e-2, # Higher learning rate than full fine-tuning.\n",
    "    num_train_epochs=5,\n",
    "    no_cuda=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_sample,\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "563ba473-ec2b-4323-b55a-5ae9fd63f579",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32mPASSED\u001B[0m: All tests passed for lesson2, question3\n\u001B[32mRESULTS RECORDED\u001B[0m: Click `Submit` when all questions are completed to log the results.\n"
     ]
    }
   ],
   "source": [
    "# Test your answer. DO NOT MODIFY THIS CELL.\n",
    "\n",
    "dbTestQuestion2_3(trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d29d98c5-6939-446a-bfeb-a751d10cddf6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5eb57ec7-7646-412f-938f-e014c315599f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Question 4 \n",
    "\n",
    "Load the PEFT model using pre-defined LoRA configs and foundation model. We set `is_trainable=False` to avoid further training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64ddfcf6-a47c-4f9c-8da9-9f826b89e806",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "time_now = time.time()\n",
    "\n",
    "username = spark.sql(\"SELECT CURRENT_USER\").first()[0]\n",
    "peft_model_path = os.path.join(output_directory, f\"peft_model_{time_now}\")\n",
    "\n",
    "trainer.model.save_pretrained(peft_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fb9aec2-8bc7-4d2c-84e9-fb470f70405c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "loaded_model = PeftModel.from_pretrained(foundation_model, \n",
    "                                        peft_model_path, \n",
    "                                        is_trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09550a3d-3833-4108-b75b-c493cbf6af39",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32mPASSED\u001B[0m: All tests passed for lesson2, question4\n\u001B[32mRESULTS RECORDED\u001B[0m: Click `Submit` when all questions are completed to log the results.\n"
     ]
    }
   ],
   "source": [
    "# Test your answer. DO NOT MODIFY THIS CELL.\n",
    "\n",
    "dbTestQuestion2_4(loaded_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c7a0fd45-fd06-4e5b-8554-7e70e6bdfaa6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8055e5e3-a735-4e40-8146-6cc1d623c01e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Question 5\n",
    "\n",
    "Generate output tokens to the same input we provided in the demo notebook before. How do the outputs compare?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a8e804b-3744-4af9-8b5b-d560d0b35949",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Two things are infinite: ”.”””””””””””””.”””””.””””.”””””””””.”””””””””””””']\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "inputs = tokenizer(\"Two things are infinite: \", return_tensors=\"pt\")\n",
    "outputs = peft_model.generate(\n",
    "    input_ids=inputs[\"input_ids\"], \n",
    "    attention_mask=inputs[\"attention_mask\"], \n",
    "    max_new_tokens=50, \n",
    "    eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "print(tokenizer.batch_decode(outputs, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5964a276-c787-4aa6-ad2f-f2870af5f7c3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32mPASSED\u001B[0m: All tests passed for lesson2, question5\n\u001B[32mRESULTS RECORDED\u001B[0m: Click `Submit` when all questions are completed to log the results.\n"
     ]
    }
   ],
   "source": [
    "# Test your answer. DO NOT MODIFY THIS CELL.\n",
    "\n",
    "dbTestQuestion2_5(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9ebea45b-f676-425f-8c78-f4ff53a1391c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "&copy; 2023 Databricks, Inc. All rights reserved.<br/>\n",
    "Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
    "<br/>\n",
    "<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "LLM 02L - LoRA with PEFT",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
