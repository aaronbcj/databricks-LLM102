# databricks-LLM102
Large Language Models: Foundation Models from the Ground Up!  

This course peels back the layers and explores the inner workings of Large Language Models (LLMs). It is intended for technical practitioners and scientists, including machine learning researchers, research scientists, machine learning engineers, and developers looking to build a strong technical and theoretical foundation in LLMs. We will navigate the breadth and depth of the inner workings of LLMs, starting from the basics of the transformer block and the attention mechanism, to the latest advances in multi-modal LLMs. We will explore the parameter-efficient fine-tuning (PEFT) methods, and other deployment strategies like quantization and Mixture-of-Experts (MoE) approaches to see how the field is advancing today.

Through this course, you'll have the opportunities to learn more about PyTorch, important LLM platforms like Hugging Face, and access to a hands-on experience with the Databricks environment. We will dive into the following topics:

* The transformer architecture and attention
* Efficient fine-tuning
* Optimizing deployment and training consideration for LLMs
* Multi-Modal LLMs
